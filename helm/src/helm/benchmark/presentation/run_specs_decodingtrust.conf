# Main `RunSpec`s for the benchmarking.

entries: [
  ##### DecodingTrust Trustworthiness Perspectives #####

  ## Toxicity  #####
  # subject: toxic, non-toxic, gpt-3.5, gpt-4

  {description: "decodingtrust_toxicity_prompts:subject=toxic,model=text", priority=1}
  {description: "decodingtrust_toxicity_prompts:subject=non-toxic,model=text", priority=1}
  {description: "decodingtrust_toxicity_prompts:subject=gpt-3.5,model=text", priority=1}
  {description: "decodingtrust_toxicity_prompts:subject=gpt-4,model=text", priority=1}

  ## Stereotype bias  #####
  # Tasks: all

  {description: "stereotype_bias:task=all,model=text", priority=1}

  ## Robustness to Adversarial Text Input #####
  # Tasks: SST-2, MNLI, MNLI(-mm), QQP, QNLI, RTE

  {description: "adv_robustness:task=sst2,model=text", priority=1}
  {description: "adv_robustness:task=qqp,model=text", priority=1}
  {description: "adv_robustness:task=mnli,model=text", priority=1}
  {description: "adv_robustness:task=mnli-mm,model=text", priority=1}
  {description: "adv_robustness:task=rte,model=text", priority=1}
  {description: "adv_robustness:task=qnli,model=text", priority=1}

  ## OOD robustness ##
  # Tasks: OOD styles (SST-2)
  {description: "ood_robustness:ood_type=style,task=shake_w,demo_name=base,run_id=-1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=augment,demo_name=base,run_id=-1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=shake_p0,demo_name=base,run_id=-1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=shake_p0.6,demo_name=base,run_id=-1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=bible_p0,demo_name=base,run_id=-1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=bible_p0.6,demo_name=base,run_id=-1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=romantic_p0,demo_name=base,run_id=-1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=romantic_p0.6,demo_name=base,run_id=-1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=tweet_p0,demo_name=base,run_id=-1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=tweet_p0.6,demo_name=base,run_id=-1,idk=0,model=text", priority=1}

  #Task: OOD in-context demonstrations (style)
  {description: "ood_robustness:ood_type=style,task=shake_p0.6,demo_name=base,run_id=0,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=shake_p0.6,demo_name=base,run_id=1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=shake_p0.6,demo_name=base,run_id=2,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=bible_p0.6,demo_name=base,run_id=0,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=bible_p0.6,demo_name=base,run_id=1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=bible_p0.6,demo_name=base,run_id=2,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=romantic_p0.6,demo_name=base,run_id=0,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=romantic_p0.6,demo_name=base,run_id=1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=romantic_p0.6,demo_name=base,run_id=2,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=tweet_p0.6,demo_name=base,run_id=0,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=tweet_p0.6,demo_name=base,run_id=1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=style,task=tweet_p0.6,demo_name=base,run_id=2,idk=0,model=text", priority=1}

  #Task: OOD knowledge
  {description: "ood_robustness:ood_type=knowledge,task=qa_2023,demo_name=2021_qa,run_id=-1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=knowledge,task=qa_2023,demo_name=2021_qa,run_id=-1,idk=1,model=text", priority=1}

  #Task: OOD in-context demonstrations (domain)
  {description: "ood_robustness:ood_type=knowledge,task=qa_2020,demo_name=global_facts,run_id=0,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=knowledge,task=qa_2020,demo_name=global_facts,run_id=1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=knowledge,task=qa_2020,demo_name=global_facts,run_id=2,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=knowledge,task=qa_2020,demo_name=machine_learning,run_id=0,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=knowledge,task=qa_2020,demo_name=machine_learning,run_id=1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=knowledge,task=qa_2020,demo_name=machine_learning,run_id=2,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=knowledge,task=qa_2020,demo_name=moral_scenarios,run_id=0,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=knowledge,task=qa_2020,demo_name=moral_scenarios,run_id=1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=knowledge,task=qa_2020,demo_name=moral_scenarios,run_id=2,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=knowledge,task=qa_2020,demo_name=us_foreign_policy,run_id=0,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=knowledge,task=qa_2020,demo_name=us_foreign_policy,run_id=1,idk=0,model=text", priority=1}
  {description: "ood_robustness:ood_type=knowledge,task=qa_2020,demo_name=us_foreign_policy,run_id=2,idk=0,model=text", priority=1}

  ### Adv_demonstration ###
  # for counterfactual 
  {description: "adv_demonstration:perspective=counterfactual,data=snli_premise,demo_name=demo+cf,description=nli3,model=text", priority=1}
  {description: "adv_demonstration:perspective=counterfactual,data=snli_hypothesis,demo_name=demo+cf,description=nli3,model=text", priority=1}
  {description: "adv_demonstration:perspective=counterfactual,data=control_raising,demo_name=demo+cf,description=control_raising,model=text", priority=1}
  {description: "adv_demonstration:perspective=counterfactual,data=irregular_form,demo_name=demo+cf,description=irregular_form,model=text", priority=1}
  {description: "adv_demonstration:perspective=counterfactual,data=main_verb,demo_name=demo+cf,description=main_verb,model=text", priority=1}
  {description: "adv_demonstration:perspective=counterfactual,data=syntactic_category,demo_name=demo+cf,description=syntactic_category,model=text", priority=1}
  # for spurious
  {description: "adv_demonstration:perspective=spurious,data=PP,demo_name=entail-bias,description=nli2,model=text", priority=1}
  {description: "adv_demonstration:perspective=spurious,data=adverb,demo_name=entail-bias,description=nli2,model=text", priority=1}
  {description: "adv_demonstration:perspective=spurious,data=embedded_under_verb,demo_name=entail-bias,description=nli2,model=text", priority=1}
  {description: "adv_demonstration:perspective=spurious,data=l_relative_clause,demo_name=entail-bias,description=nli2,model=text", priority=1}
  {description: "adv_demonstration:perspective=spurious,data=passive,demo_name=entail-bias,description=nli2,model=text", priority=1}
  {description: "adv_demonstration:perspective=spurious,data=s_relative_clause,demo_name=entail-bias,description=nli2,model=text", priority=1}
  {description: "adv_demonstration:perspective=spurious,data=PP,demo_name=non-entail-bias,description=nli2,model=text", priority=1}
  {description: "adv_demonstration:perspective=spurious,data=adverb,demo_name=non-entail-bias,description=nli2,model=text", priority=1}
  {description: "adv_demonstration:perspective=spurious,data=embedded_under_verb,demo_name=non-entail-bias,description=nli2,model=text", priority=1}
  {description: "adv_demonstration:perspective=spurious,data=l_relative_clause,demo_name=non-entail-bias,description=nli2,model=text", priority=1}
  {description: "adv_demonstration:perspective=spurious,data=passive,demo_name=non-entail-bias,description=nli2,model=text", priority=1}
  {description: "adv_demonstration:perspective=spurious,data=s_relative_clause,demo_name=non-entail-bias,description=nli2,model=text", priority=1}
  # for backdoor
  {description: "adv_demonstration:perspective=backdoor,data=sst-2,demo_name=setup1_badword,description=sa,model=text", priority=1}
  {description: "adv_demonstration:perspective=backdoor,data=sst-2,demo_name=setup2_badword,description=sa,model=text", priority=1}
  {description: "adv_demonstration:perspective=backdoor,data=sst-2,demo_name=setup3_badword,description=sa,model=text", priority=1}
  {description: "adv_demonstration:perspective=backdoor,data=sst-2,demo_name=setup1_addsent,description=sa,model=text", priority=1}
  {description: "adv_demonstration:perspective=backdoor,data=sst-2,demo_name=setup2_addsent,description=sa,model=text", priority=1}
  {description: "adv_demonstration:perspective=backdoor,data=sst-2,demo_name=setup3_addsent,description=sa,model=text", priority=1}
  {description: "adv_demonstration:perspective=backdoor,data=sst-2,demo_name=setup1_synbkd,description=sa,model=text", priority=1}
  {description: "adv_demonstration:perspective=backdoor,data=sst-2,demo_name=setup2_synbkd,description=sa,model=text", priority=1}
  {description: "adv_demonstration:perspective=backdoor,data=sst-2,demo_name=setup3_synbkd,description=sa,model=text", priority=1}
  {description: "adv_demonstration:perspective=backdoor,data=sst-2,demo_name=setup1_stylebkd,description=sa,model=text", priority=1}
  {description: "adv_demonstration:perspective=backdoor,data=sst-2,demo_name=setup2_stylebkd,description=sa,model=text", priority=1}
  {description: "adv_demonstration:perspective=backdoor,data=sst-2,demo_name=setup3_stylebkd,description=sa,model=text", priority=1}


  ### Privacy ###
  # Task: privacy understanding 
  
  {description: "privacy:scenario_name=privacy_understanding,data_file='',template=0,few_shot_num=0,dataset_size=100,prompt_type=1,model=text", priority=1}
  {description: "privacy:scenario_name=privacy_understanding,data_file='',template=0,few_shot_num=0,dataset_size=100,prompt_type=2,model=text", priority=1}
  {description: "privacy:scenario_name=privacy_understanding,data_file='',template=0,few_shot_num=0,dataset_size=100,prompt_type=3,model=text", priority=1}
  
  # Task: privacy pii
  {description: "privacy:scenario_name=pii,data_file=enron_data/five_shot.pkl,template=1,few_shot_num=0,dataset_size=100,prompt_type=attack,model=text", priority=1}
  {description: "privacy:scenario_name=pii,data_file=enron_data/five_shot.pkl,template=1,few_shot_num=3,dataset_size=100,prompt_type=attack,model=text", priority=1}
  {description: "privacy:scenario_name=pii,data_file=enron_data/five_shot.pkl,template=1,few_shot_num=3,dataset_size=100,prompt_type=protect,model=text", priority=1}

  # task: train data extraction on enron email
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=0,dataset_size=0,prompt_type=context-50,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=0,dataset_size=0,prompt_type=context-100,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=0,dataset_size=0,prompt_type=context-200,model=text", priority=1}

  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=5,dataset_size=0,prompt_type=five-shot-a,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=5,dataset_size=0,prompt_type=five-shot-b,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=5,dataset_size=0,prompt_type=five-shot-c,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=5,dataset_size=0,prompt_type=five-shot-d,model=text", priority=1}

  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=1,dataset_size=0,prompt_type=one-shot-a,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=1,dataset_size=0,prompt_type=one-shot-b,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=1,dataset_size=0,prompt_type=one-shot-c,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=1,dataset_size=0,prompt_type=one-shot-d,model=text", priority=1}

  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=0,dataset_size=0,prompt_type=knownzero-shot-a,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=0,dataset_size=0,prompt_type=knownzero-shot-b,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=0,dataset_size=0,prompt_type=knownzero-shot-c,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=0,dataset_size=0,prompt_type=knownzero-shot-d,model=text", priority=1}

  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=5,dataset_size=0,prompt_type=five-shot-non-domain-a,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=5,dataset_size=0,prompt_type=five-shot-non-domain-b,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=5,dataset_size=0,prompt_type=five-shot-non-domain-c,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=5,dataset_size=0,prompt_type=five-shot-non-domain-d,model=text", priority=1}

  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=1,dataset_size=0,prompt_type=one-shot-non-domain-a,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=1,dataset_size=0,prompt_type=one-shot-non-domain-b,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=1,dataset_size=0,prompt_type=one-shot-non-domain-c,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=1,dataset_size=0,prompt_type=one-shot-non-domain-d,model=text", priority=1}

  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=0,dataset_size=0,prompt_type=zero-shot-a,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=0,dataset_size=0,prompt_type=zero-shot-b,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=0,dataset_size=0,prompt_type=zero-shot-c,model=text", priority=1}
  {description: "privacy:scenario_name=enron_email_extraction,data_file=enron_data,template=0,few_shot_num=0,dataset_size=0,prompt_type=zero-shot-d,model=text", priority=1}


  ### Machine Ethics ###
  # Machine Ethics: evaluation on standard benchmarks
  {description: "machine_ethics:data_name=ethics_commonsense_short,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=0,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=0,model=text", priority=1} # Evaluation on short commonsense morality subset of ETHICS dataset under zero-shot setting
  {description: "machine_ethics:data_name=ethics_commonsense_short,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=0,few_shot_num=32,jailbreak_prompt=0,evasive_sentence=0,model=text", priority=1} # Evaluation on short commonsense morality subset of ETHICS dataset under few-shot setting
  {description: "machine_ethics:data_name=ethics_commonsense_long,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=0,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=0,model=text", priority=1} # Evaluation on long commonsense morality subset of ETHICS dataset under zero-shot setting
  {description: "machine_ethics:data_name=ethics_commonsense_long,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=0,few_shot_num=8,jailbreak_prompt=0,evasive_sentence=0,model=text", priority=1} # Evaluation on long commonsense morality subset of ETHICS dataset under few-shot setting
  {description: "machine_ethics:data_name=jiminy,test_data_file=jiminy_test.json,train_data_file=jiminy_train.json,test_num=0,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=0,model=text", priority=1} # Evaluation on Jiminy Cricket dataset under zero-shot setting
  {description: "machine_ethics:data_name=jiminy,test_data_file=jiminy_test.json,train_data_file=jiminy_train.json,test_num=0,few_shot_num=3,jailbreak_prompt=0,evasive_sentence=0,model=text", priority=1} # Evaluation on Jiminy Cricket dataset under few-shot setting
  
  # Machine Ethics: evaluation on jailbreaking prompts
  {description: "machine_ethics:data_name=ethics_commonsense_short,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=200,few_shot_num=0,jailbreak_prompt=1,evasive_sentence=0,model=text", priority=1} # Evaluation on jailbreaking prompts on ETHICS dataset
  {description: "machine_ethics:data_name=ethics_commonsense_short,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=200,few_shot_num=0,jailbreak_prompt=2,evasive_sentence=0,model=text", priority=1} # Evaluation on jailbreaking prompts on ETHICS dataset
  {description: "machine_ethics:data_name=ethics_commonsense_short,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=200,few_shot_num=0,jailbreak_prompt=3,evasive_sentence=0,model=text", priority=1} # Evaluation on jailbreaking prompts on ETHICS dataset 
  {description: "machine_ethics:data_name=ethics_commonsense_short,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=200,few_shot_num=0,jailbreak_prompt=4,evasive_sentence=0,model=text", priority=1} # Evaluation on jailbreaking prompts on ETHICS dataset
  {description: "machine_ethics:data_name=ethics_commonsense_short,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=200,few_shot_num=0,jailbreak_prompt=5,evasive_sentence=0,model=text", priority=1} # Evaluation on jailbreaking prompts on ETHICS dataset
  
  {description: "machine_ethics:data_name=jiminy,test_data_file=jiminy_test.json,train_data_file=jiminy_train.json,test_num=200,few_shot_num=0,jailbreak_prompt=1,evasive_sentence=0,model=text", priority=1} # Evaluation on jailbreaking prompts on Jiminy Cricket dataset
  {description: "machine_ethics:data_name=jiminy,test_data_file=jiminy_test.json,train_data_file=jiminy_train.json,test_num=200,few_shot_num=0,jailbreak_prompt=2,evasive_sentence=0,model=text", priority=1} # Evaluation on jailbreaking prompts on Jiminy Cricket dataset
  {description: "machine_ethics:data_name=jiminy,test_data_file=jiminy_test.json,train_data_file=jiminy_train.json,test_num=200,few_shot_num=0,jailbreak_prompt=3,evasive_sentence=0,model=text", priority=1} # Evaluation on jailbreaking prompts on Jiminy Cricket dataset
  {description: "machine_ethics:data_name=jiminy,test_data_file=jiminy_test.json,train_data_file=jiminy_train.json,test_num=200,few_shot_num=0,jailbreak_prompt=4,evasive_sentence=0,model=text", priority=1} # Evaluation on jailbreaking prompts on Jiminy Cricket dataset
  {description: "machine_ethics:data_name=jiminy,test_data_file=jiminy_test.json,train_data_file=jiminy_train.json,test_num=200,few_shot_num=0,jailbreak_prompt=5,evasive_sentence=0,model=text", priority=1} # Evaluation on jailbreaking prompts on Jiminy Cricket dataset
  
  # Machine Ethics: evaluation on evasive sentences
  {description: "machine_ethics:data_name=ethics_commonsense_short,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=200,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=1,model=text", priority=1} # Evaluation on jailbreaking prompts on ETHICS dataset
  {description: "machine_ethics:data_name=ethics_commonsense_short,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=200,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=2,model=text", priority=1} # Evaluation on jailbreaking prompts on ETHICS dataset
  {description: "machine_ethics:data_name=ethics_commonsense_short,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=200,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=3,model=text", priority=1} # Evaluation on jailbreaking prompts on ETHICS dataset
  {description: "machine_ethics:data_name=ethics_commonsense_short,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=200,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=4,model=text", priority=1} # Evaluation on jailbreaking prompts on ETHICS dataset
  {description: "machine_ethics:data_name=ethics_commonsense_short,test_data_file=ethics/commonsense/cm_test.csv,train_data_file=ethics/commonsense/cm_train.csv,test_num=200,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=5,model=text", priority=1} # Evaluation on jailbreaking prompts on ETHICS dataset
  
  {description: "machine_ethics:data_name=jiminy,test_data_file=jiminy_test.json,train_data_file=jiminy_train.json,test_num=200,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=1,model=text", priority=1} # Evaluation on jailbreaking prompts on Jiminy Cricket dataset
  {description: "machine_ethics:data_name=jiminy,test_data_file=jiminy_test.json,train_data_file=jiminy_train.json,test_num=200,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=2,model=text", priority=1} # Evaluation on jailbreaking prompts on Jiminy Cricket dataset
  {description: "machine_ethics:data_name=jiminy,test_data_file=jiminy_test.json,train_data_file=jiminy_train.json,test_num=200,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=3,model=text", priority=1} # Evaluation on jailbreaking prompts on Jiminy Cricket dataset
  {description: "machine_ethics:data_name=jiminy,test_data_file=jiminy_test.json,train_data_file=jiminy_train.json,test_num=200,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=4,model=text", priority=1} # Evaluation on jailbreaking prompts on Jiminy Cricket dataset
  {description: "machine_ethics:data_name=jiminy,test_data_file=jiminy_test.json,train_data_file=jiminy_train.json,test_num=200,few_shot_num=0,jailbreak_prompt=0,evasive_sentence=5,model=text", priority=1} # Evaluation on jailbreaking prompts on Jiminy Cricket dataset

  ### Fairness ###
  # Fairness: evaluation in zero-shot setting
  {description: "fairness:task=adult,train_br=0.0,test_br=0.0,num_train=0,num_test=200,model=text", priority=1} # evaluation in zero-shot setting on test sets with base rate parity difference 0.0
  {description: "fairness:task=adult,train_br=0.0,test_br=0.5,num_train=0,num_test=200,model=text", priority=1} # evaluation in zero-shot setting on test sets with base rate parity difference 0.5
  {description: "fairness:task=adult,train_br=0.0,test_br=1.0,num_train=0,num_test=200,,model=text", priority=1} # evaluation in zero-shot setting on test sets with base rate parity difference 1.0

  # Fairness: evaluation in few-shot setting with different demographically imbalanced context
  {description: "fairness:task=adult,train_br=0.0,test_br=0.0,num_train=32,num_test=200,model=text", priority=1} # evaluation in few-shot setting with context of base rate parity difference 0.0
  {description: "fairness:task=adult,train_br=0.5,test_br=0.0,num_train=32,num_test=200,model=text", priority=1} # evaluation in few-shot setting with context of base rate parity difference 0.5
  {description: "fairness:task=adult,train_br=1.0,test_br=0.0,num_train=32,num_test=200,model=text", priority=1} # evaluation in few-shot setting with context of base rate parity difference 1.0

  # Fairness: evaluation in few-shot setting with different numbers of fair few-shot examples
  {description: "fairness:task=adult,train_br=0.0,test_br=0.5,num_train=0,num_test=200,model=text", priority=1} # evaluation in few-shot setting with context of 0 fair few-shot examples
  {description: "fairness:task=adult,train_br=0.0,test_br=0.5,num_train=16,num_test=200,model=text", priority=1} # evaluation in few-shot setting with context of 16 fair few-shot examples
  {description: "fairness:task=adult,train_br=0.0,test_br=0.5,num_train=32,num_test=200,model=text", priority=1} # evaluation in few-shot setting with context of 32 fair few-shot examples
]
